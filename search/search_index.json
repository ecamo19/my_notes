{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Notes My notes","title":"Notes"},{"location":"#notes","text":"My notes","title":"Notes"},{"location":"code_archive/python/concepts_python/","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"code_archive/python/concepts_python/#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"code_archive/python/concepts_python/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"code_archive/python/concepts_python/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"linux/docker/concepts_docker/","text":"Docker Project layout","title":"Docker"},{"location":"linux/docker/concepts_docker/#docker","text":"","title":"Docker"},{"location":"linux/docker/concepts_docker/#_1","text":"","title":""},{"location":"linux/docker/concepts_docker/#project-layout","text":"","title":"Project layout"},{"location":"stats_and_modeling/bias_variance_tradeoff/","text":"Bias-Variance trade-off In a perfect scenario, a statistical method should achieve low variance and low bias Bias Bias is defined as the error introduced by approximating a real life problem (which in nature could have complicated relationships) by a simple model I associate bias as a model simplification. Like Fitting a linear model that follow a non-lineal trend Variance How much a function $f$ would change if we estimated it using a different train set? High variance indicate that if we change a data point in the data the function $f$ will change. I associate variance with overfit. Models with high variance have high flexibility because those model adjust following each data point. For example linear models have low variance but they could have high bias Highly correlated predictors can lead to collinearity issues and this can greatly increase the model variance. More complex models can have very high variance, which leads to over-\ufb01tting. On the other hand, simple models tend not to over-\ufb01t, but under-\ufb01t if they are not \ufb02exible enough to model the true relationship (thus high bias).","title":"Bias-Variance trade-off"},{"location":"stats_and_modeling/bias_variance_tradeoff/#bias-variance-trade-off","text":"In a perfect scenario, a statistical method should achieve low variance and low bias Bias Bias is defined as the error introduced by approximating a real life problem (which in nature could have complicated relationships) by a simple model I associate bias as a model simplification. Like Fitting a linear model that follow a non-lineal trend Variance How much a function $f$ would change if we estimated it using a different train set? High variance indicate that if we change a data point in the data the function $f$ will change. I associate variance with overfit. Models with high variance have high flexibility because those model adjust following each data point. For example linear models have low variance but they could have high bias Highly correlated predictors can lead to collinearity issues and this can greatly increase the model variance. More complex models can have very high variance, which leads to over-\ufb01tting. On the other hand, simple models tend not to over-\ufb01t, but under-\ufb01t if they are not \ufb02exible enough to model the true relationship (thus high bias).","title":"Bias-Variance trade-off"},{"location":"stats_and_modeling/dimension_reduction/","text":"Dimension Reduction Methods When n << p and/or there is collinearity the variance will increase so OLS will not perform well, so other methods are need to overcome this. These methods control variance by transforming the predictors and then fit a least squares model using these transformed variables. With these methods overfitting is avoid by using a less flexible approach. If the correlation among predictors is high, then the ordinary least squares solution for multiple linear regression will have high variability and will become unstable OR if n << p least squares will be unable to \ufb01nd a unique set of regression coe\ufb03cients that minimize the SSE PCR Does not perform feature selection Sometimes dimension reduction via PCA does not necessarily produce new predictors that explain the response. The scores indicates if a value is below or above average. For example if the score < 0 in a PCA with two variables, then this indicates a city with below-average population size and below average ad spending. PLS Kuhn: We recommend using PLS when there are correlated predictors and a linear regression-type solution is desired PLS identifies new features in a supervised way by making use of the response Y in order to identify new features that not only approximate the old features well, but also that are related to the response. PLS \ufb01nds linear combinations of the predictors What Goes Wrong in High Dimensions? Cp, AIC, and BIC approaches are not appropriate in the high-dimensional setting, because estimating $\\sigma^2$ is problematic. PCR vrs PLS Since the directions in PLS are obtained by integrating the response variable, they explain more variation in Y than the principal components regression do. PLS also gives similar loadings to correlated variables, but variables that are correlated with the Y receive more weights compared with PCR.","title":"Dimension Reduction Methods"},{"location":"stats_and_modeling/dimension_reduction/#dimension-reduction-methods","text":"When n << p and/or there is collinearity the variance will increase so OLS will not perform well, so other methods are need to overcome this. These methods control variance by transforming the predictors and then fit a least squares model using these transformed variables. With these methods overfitting is avoid by using a less flexible approach. If the correlation among predictors is high, then the ordinary least squares solution for multiple linear regression will have high variability and will become unstable OR if n << p least squares will be unable to \ufb01nd a unique set of regression coe\ufb03cients that minimize the SSE","title":"Dimension Reduction Methods"},{"location":"stats_and_modeling/dimension_reduction/#pcr","text":"Does not perform feature selection Sometimes dimension reduction via PCA does not necessarily produce new predictors that explain the response. The scores indicates if a value is below or above average. For example if the score < 0 in a PCA with two variables, then this indicates a city with below-average population size and below average ad spending.","title":"PCR"},{"location":"stats_and_modeling/dimension_reduction/#pls","text":"Kuhn: We recommend using PLS when there are correlated predictors and a linear regression-type solution is desired PLS identifies new features in a supervised way by making use of the response Y in order to identify new features that not only approximate the old features well, but also that are related to the response. PLS \ufb01nds linear combinations of the predictors","title":"PLS"},{"location":"stats_and_modeling/dimension_reduction/#what-goes-wrong-in-high-dimensions","text":"Cp, AIC, and BIC approaches are not appropriate in the high-dimensional setting, because estimating $\\sigma^2$ is problematic.","title":"What Goes Wrong in High Dimensions?"},{"location":"stats_and_modeling/dimension_reduction/#pcr-vrs-pls","text":"Since the directions in PLS are obtained by integrating the response variable, they explain more variation in Y than the principal components regression do. PLS also gives similar loadings to correlated variables, but variables that are correlated with the Y receive more weights compared with PCR.","title":"PCR vrs PLS"},{"location":"stats_and_modeling/model_selection/","text":"Model selection Best Subset Selection Fit a separate least squares regression for each possible combination of $p$ predictors. That is, fit all possible combinations and evaluate all the possibilities. The problem with this is that we have to evaluate $2^p$ models which is computationally expensive. Forward Stepwise Selection This method starts with a null model with no predictors, then add a predictor, evaluate the model and repeats. Forward selection can be applied when n < p but only to $M_0$ ..... $M_{n - 1}$ since n >= p will not yield a unique solution. Backward Stepwise Selection This method begins with the full least squares model containing all p predictors. This works when n > p Choosing the Optimal Model Cp , AIC, BIC, and Adjusted R^2 Cp: Add a 2d$\\sigma^2$ penalty that increases as the number of predictor increase. Cp ~ AIC BIC: Usually BIC chooses a model with less predictors that the one chosen by Cp One-standard-error rule. We first calculate the standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve. Do not use R^2 or RSS for selecting a model The training error will decrease as more variables are included in the model, but the test error may not. Therefore, training set RSS and training set R^2 cannot be used to select from among a set of models with di\ufb00erent numbers of variables. $R^2$ increases with the number of predictors while RSS decrease.","title":"Model selection"},{"location":"stats_and_modeling/model_selection/#model-selection","text":"","title":"Model selection"},{"location":"stats_and_modeling/model_selection/#best-subset-selection","text":"Fit a separate least squares regression for each possible combination of $p$ predictors. That is, fit all possible combinations and evaluate all the possibilities. The problem with this is that we have to evaluate $2^p$ models which is computationally expensive.","title":"Best Subset Selection"},{"location":"stats_and_modeling/model_selection/#forward-stepwise-selection","text":"This method starts with a null model with no predictors, then add a predictor, evaluate the model and repeats. Forward selection can be applied when n < p but only to $M_0$ ..... $M_{n - 1}$ since n >= p will not yield a unique solution.","title":"Forward Stepwise Selection"},{"location":"stats_and_modeling/model_selection/#backward-stepwise-selection","text":"This method begins with the full least squares model containing all p predictors. This works when n > p","title":"Backward Stepwise Selection"},{"location":"stats_and_modeling/model_selection/#choosing-the-optimal-model-cp-aic-bic-and-adjusted-r2","text":"Cp: Add a 2d$\\sigma^2$ penalty that increases as the number of predictor increase. Cp ~ AIC BIC: Usually BIC chooses a model with less predictors that the one chosen by Cp","title":"Choosing the Optimal Model Cp , AIC, BIC, and Adjusted R^2"},{"location":"stats_and_modeling/model_selection/#one-standard-error-rule","text":"We first calculate the standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve.","title":"One-standard-error rule."},{"location":"stats_and_modeling/model_selection/#do-not-use-r2-or-rss-for-selecting-a-model","text":"The training error will decrease as more variables are included in the model, but the test error may not. Therefore, training set RSS and training set R^2 cannot be used to select from among a set of models with di\ufb00erent numbers of variables. $R^2$ increases with the number of predictors while RSS decrease.","title":"Do not use R^2 or RSS for selecting a model"},{"location":"stats_and_modeling/regularization/","text":"Penalized Models When n << p and/or there is collinearity the variance will increase so OLS will not perform well, so other methods are need to overcome this. Penalized methods work by controlling variance (less flexible methods) by shrinking coefs towards zero or exactly zero. With these methods overfitting is avoid by using a less flexible approach. By sacri\ufb01cing some bias, we can often reduce the variance enough to make the overall MSE lower than unbiased models. Ridge Regression Ridge Regression will include all p predictors in the final model. This method just shrink coefs towards zero. Ridge regression (Hoerl 1970) adds a penalty on the sum of the squared regression parameters (l2) $\\lambda$ = Penalty term (l2), the larger the $\\lambda$ is the smallest (towards zero) the coefs are. Ridge regression\u2019s advantage over least squares is rooted in the bias-variance trade-o\ufb00. As $\\lambda$ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias. At some point MSE will decrease. Ridge regression works best in situations where the least squares estimates have high variance. Lasso Regression The lasso shrinks the coefs to zero which can be used for variable selection Differences between lasso and ridge Penalty term Lasso performs feature selection Lasso will perform better in a scenario where a few p have a high coefs and the others are close to zero Ridge will perform better when there are many predictors with with coefs of the same ~size","title":"Penalized Models"},{"location":"stats_and_modeling/regularization/#penalized-models","text":"When n << p and/or there is collinearity the variance will increase so OLS will not perform well, so other methods are need to overcome this. Penalized methods work by controlling variance (less flexible methods) by shrinking coefs towards zero or exactly zero. With these methods overfitting is avoid by using a less flexible approach. By sacri\ufb01cing some bias, we can often reduce the variance enough to make the overall MSE lower than unbiased models.","title":"Penalized Models"},{"location":"stats_and_modeling/regularization/#ridge-regression","text":"Ridge Regression will include all p predictors in the final model. This method just shrink coefs towards zero. Ridge regression (Hoerl 1970) adds a penalty on the sum of the squared regression parameters (l2) $\\lambda$ = Penalty term (l2), the larger the $\\lambda$ is the smallest (towards zero) the coefs are. Ridge regression\u2019s advantage over least squares is rooted in the bias-variance trade-o\ufb00. As $\\lambda$ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias. At some point MSE will decrease. Ridge regression works best in situations where the least squares estimates have high variance.","title":"Ridge Regression"},{"location":"stats_and_modeling/regularization/#lasso-regression","text":"The lasso shrinks the coefs to zero which can be used for variable selection Differences between lasso and ridge Penalty term Lasso performs feature selection Lasso will perform better in a scenario where a few p have a high coefs and the others are close to zero Ridge will perform better when there are many predictors with with coefs of the same ~size","title":"Lasso Regression"},{"location":"stats_and_modeling/resampling_methods/","text":"Resampling Methods Resampling methods are used in the absence of very large designated test sets that can be used to directly estimate test error rates Validation Set Approach Good when n is large. This method consist in dividing randomly the data set into Training and Test (Validation) data sets Drawbacks: 1) High Variable, depends on the observations chosen 2) Not all the data is included in training the model Leave-one-out cross validation (LOOCV) In this method a single pair observations ($x_1$,$y_1$) are left out for validation and the rest of the data is used for training being repeated n times Drawbacks: 1) Expensive to implement (time consuming, the validation is done in n observations) k-fold Cross Validation Randomly divide the whole data set in k groups of equal size. The first fold is used as validation set and the others are used as training sets. This procedure is repeated k times, each time, a different fold is treated as validation test. For example if k = 10 then the data set is divided into 10 folds, the first one will be used as validation set and then the statistical method (i.e. linear model) is fitted on each one of the other 9 folds. This is repeated until each one of the folds has being used as validation sets.","title":"Resampling Methods"},{"location":"stats_and_modeling/resampling_methods/#resampling-methods","text":"Resampling methods are used in the absence of very large designated test sets that can be used to directly estimate test error rates Validation Set Approach Good when n is large. This method consist in dividing randomly the data set into Training and Test (Validation) data sets Drawbacks: 1) High Variable, depends on the observations chosen 2) Not all the data is included in training the model Leave-one-out cross validation (LOOCV) In this method a single pair observations ($x_1$,$y_1$) are left out for validation and the rest of the data is used for training being repeated n times Drawbacks: 1) Expensive to implement (time consuming, the validation is done in n observations) k-fold Cross Validation Randomly divide the whole data set in k groups of equal size. The first fold is used as validation set and the others are used as training sets. This procedure is repeated k times, each time, a different fold is treated as validation test. For example if k = 10 then the data set is divided into 10 folds, the first one will be used as validation set and then the statistical method (i.e. linear model) is fitted on each one of the other 9 folds. This is repeated until each one of the folds has being used as validation sets.","title":"Resampling Methods"}]}